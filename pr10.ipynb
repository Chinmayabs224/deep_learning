{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71414bff",
   "metadata": {},
   "source": [
    "Aim:  Study the effect of batch normalization and dropout in neural network \n",
    "classifier\n",
    "\n",
    " Procedure:\n",
    " 1. Load and Preprocess the Data:\n",
    "    ● Load the MNIST dataset.\n",
    "    ● Normalize the pixel values of the images to be between 0 and 1.\n",
    " 2. Build the Neural Network Model\n",
    " 3. Compile the Model\n",
    " 4. Train the Model\n",
    " 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0fa0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9065 - loss: 0.3022 - val_accuracy: 0.9845 - val_loss: 0.0519\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0419 - val_accuracy: 0.9888 - val_loss: 0.0314\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9911 - loss: 0.0277 - val_accuracy: 0.9879 - val_loss: 0.0391\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9949 - loss: 0.0174 - val_accuracy: 0.9921 - val_loss: 0.0246\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9962 - loss: 0.0125 - val_accuracy: 0.9893 - val_loss: 0.0365\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9965 - loss: 0.0107 - val_accuracy: 0.9913 - val_loss: 0.0363\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.0088 - val_accuracy: 0.9899 - val_loss: 0.0365\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0082 - val_accuracy: 0.9899 - val_loss: 0.0413\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9975 - loss: 0.0066 - val_accuracy: 0.9906 - val_loss: 0.0401\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0041 - val_accuracy: 0.9901 - val_loss: 0.0437\n",
      "\n",
      "Training BatchNorm Only...\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.9488 - loss: 0.1679 - val_accuracy: 0.9621 - val_loss: 0.1222\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9876 - loss: 0.0381 - val_accuracy: 0.9895 - val_loss: 0.0323\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9912 - loss: 0.0277 - val_accuracy: 0.9896 - val_loss: 0.0349\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9946 - loss: 0.0166 - val_accuracy: 0.9881 - val_loss: 0.0378\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9945 - loss: 0.0176 - val_accuracy: 0.9909 - val_loss: 0.0323\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 0.9904 - val_loss: 0.0325\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 0.9884 - val_loss: 0.0436\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0106 - val_accuracy: 0.9903 - val_loss: 0.0334\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0095 - val_accuracy: 0.9922 - val_loss: 0.0299\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 0.9919 - val_loss: 0.0337\n",
      "\n",
      "Training Dropout Only...\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8376 - loss: 0.5015 - val_accuracy: 0.9850 - val_loss: 0.0498\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9680 - loss: 0.1065 - val_accuracy: 0.9855 - val_loss: 0.0420\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9750 - loss: 0.0810 - val_accuracy: 0.9898 - val_loss: 0.0311\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9808 - loss: 0.0647 - val_accuracy: 0.9910 - val_loss: 0.0286\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9821 - loss: 0.0590 - val_accuracy: 0.9902 - val_loss: 0.0263\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9837 - loss: 0.0544 - val_accuracy: 0.9908 - val_loss: 0.0254\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9867 - loss: 0.0445 - val_accuracy: 0.9920 - val_loss: 0.0257\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9859 - loss: 0.0450 - val_accuracy: 0.9928 - val_loss: 0.0214\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0418 - val_accuracy: 0.9929 - val_loss: 0.0203\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9874 - loss: 0.0412 - val_accuracy: 0.9931 - val_loss: 0.0226\n",
      "\n",
      "Training Both...\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.8800 - loss: 0.3851 - val_accuracy: 0.9845 - val_loss: 0.0453\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9692 - loss: 0.0989 - val_accuracy: 0.9884 - val_loss: 0.0340\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9755 - loss: 0.0780 - val_accuracy: 0.9896 - val_loss: 0.0322\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9811 - loss: 0.0617 - val_accuracy: 0.9898 - val_loss: 0.0297\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9825 - loss: 0.0559 - val_accuracy: 0.9907 - val_loss: 0.0258\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9827 - loss: 0.0524 - val_accuracy: 0.9913 - val_loss: 0.0264\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9849 - loss: 0.0482 - val_accuracy: 0.9916 - val_loss: 0.0225\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9858 - loss: 0.0437 - val_accuracy: 0.9915 - val_loss: 0.0249\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9881 - loss: 0.0397 - val_accuracy: 0.9931 - val_loss: 0.0214\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9892 - loss: 0.0350 - val_accuracy: 0.9924 - val_loss: 0.0222\n",
      "\n",
      "Test Accuracies:\n",
      "Baseline: 0.9901\n",
      "BatchNorm Only: 0.9919\n",
      "Dropout Only: 0.9931\n",
      "Both: 0.9924\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "x_train = x_train[..., np.newaxis]  # Add channel dimension\n",
    "x_test = x_test[..., np.newaxis]\n",
    "\n",
    "def create_model(use_batchnorm=True, use_dropout=True):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.BatchNormalization() if use_batchnorm else layers.Lambda(lambda x: x),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25) if use_dropout else layers.Lambda(lambda x: x),\n",
    "        \n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.BatchNormalization() if use_batchnorm else layers.Lambda(lambda x: x),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25) if use_dropout else layers.Lambda(lambda x: x),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization() if use_batchnorm else layers.Lambda(lambda x: x),\n",
    "        layers.Dropout(0.5) if use_dropout else layers.Lambda(lambda x: x),\n",
    "        layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Experiment configurations\n",
    "configs = [\n",
    "    {'name': 'Baseline', 'batchnorm': False, 'dropout': False},\n",
    "    {'name': 'BatchNorm Only', 'batchnorm': True, 'dropout': False},\n",
    "    {'name': 'Dropout Only', 'batchnorm': False, 'dropout': True},\n",
    "    {'name': 'Both', 'batchnorm': True, 'dropout': True}\n",
    "]\n",
    "\n",
    "# Train and evaluate each configuration\n",
    "results = {}\n",
    "for cfg in configs:\n",
    "    print(f\"\\nTraining {cfg['name']}...\")\n",
    "    model = create_model(cfg['batchnorm'], cfg['dropout'])\n",
    "    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=1)\n",
    "    results[cfg['name']] = history.history\n",
    "\n",
    "# Print final accuracies\n",
    "print(\"\\nTest Accuracies:\")\n",
    "for name in results:\n",
    "    acc = results[name]['val_accuracy'][-1]\n",
    "    print(f\"{name}: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
